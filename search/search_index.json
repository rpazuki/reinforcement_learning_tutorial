{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Reinforcement Learning documentation! Description Examples that describe and display different concept in reinforcement learning =============== Bandit examples: Notebook =============== Dynamic programming examples: Notebook1:Simple square scape Notebook2: Car rental policy iteration Notebook3: Car rental value iteration ===============","title":"Reinforcement Learning documentation!"},{"location":"#reinforcement-learning-documentation","text":"","title":"Reinforcement Learning documentation!"},{"location":"#description","text":"Examples that describe and display different concept in reinforcement learning ===============","title":"Description"},{"location":"#bandit-examples","text":"Notebook ===============","title":"Bandit examples:"},{"location":"#dynamic-programming-examples","text":"Notebook1:Simple square scape Notebook2: Car rental policy iteration Notebook3: Car rental value iteration ===============","title":"Dynamic programming examples:"},{"location":"bandits_package/","text":"'rl.bandits' Examples: Notebook =============== K-Arms bandit algorithms. gradient_bandit gradient_bandit ( k : int , bandits , epsilon : float = 0.1 , alpha : float = 0.1 , max_try : int = 10000 , record_stat : bool = True , ) The gradient bandit (Energy) algorithm. Parameters: k ( int ) \u2013 Number of arms. bandits \u2013 A k-arms bandits that returns rewards for actions in {0, 1, ..., k-1}. epsilon ( float , default: .1 ) \u2013 Probability of exploration. alpha ( float , default: 0.1 ) \u2013 A positive value constant step size. max_try ( int , default: 10000 ) \u2013 Maximum number of try. record_stat ( bool , default: True ) \u2013 Records the statistics and then returns it as stat dictionary content (see Returns). Returns: Hs ( ndarray ) \u2013 A one-D array of size 'k' that contains the energy of action-value. stats ( dict ) \u2013 A dictionary of recorded statistics. It will be empty when 'record_stat' is False, returns empty. The followings are the keys of the recorded stats: 'rewards' : ndarray A one-D array of size 'max_try' that contains each action's reward in order. 'Ns' : ndarray A one-D array of size 'k' that contains the number of time each action were selected. 'actions' : ndarray A one-D array of size 'max_try' that contains each actions in order. greedy_exploration greedy_exploration ( k : int , bandits , epsilon : float = 0.1 , max_try : int = 10000 , Qs_init : float = 0.0 , record_stat : bool = True , ) A greedy exploration/explotation algorithm. Parameters: k ( int ) \u2013 Number of arms. bandits \u2013 A k-arms bandits that returns rewards for actions in {0, 1, ..., k-1}. epsilon ( float , default: .1 ) \u2013 Probability of exploration. max_try ( int , default: 10000 ) \u2013 Maximum number of try. Qs_init ( int , default: 0.0 ) \u2013 Initial value of Q estimates. For optimistic initialisation, use positive values. record_stat ( bool , default: True ) \u2013 Records the statistics and then returns it as stat dictionary content (see Returns). Returns: Qs ( ndarray ) \u2013 A one-D array of size 'k' that contains the estimated expected action-value. stats ( dict ) \u2013 A dictionary of recorded statistics. It will be empty when 'record_stat' is False, returns empty. The followings are the keys of the recorded stats: 'rewards' : ndarray A one-D array of size 'max_try' that contains each action's reward in order. 'Ns' : ndarray A one-D array of size 'k' that contains the number of time each action were selected. 'actions' : ndarray A one-D array of size 'max_try' that contains each actions in order. greedy_unbiased_constant_step_size greedy_unbiased_constant_step_size ( k : int , bandits , epsilon : float = 0.1 , alpha : float = 0.1 , max_try : int = 10000 , Qs_init : float = 0.0 , record_stat : bool = True , ) An unbiased constant step exploration/explotation algorithm. Since sample averaged action-value's estimates are poorly performs on nonstationary problems, while do not produce initil bias, in this algorithm we update the step size by using a trace of rewards during the run. Parameters: k ( int ) \u2013 Number of arms. bandits \u2013 A k-arms bandits that returns rewards for actions in {0, 1, ..., k-1}. epsilon ( float , default: .1 ) \u2013 Probability of exploration. alpha ( float , default: 0.1 ) \u2013 A positive value constant step size. max_try ( int , default: 10000 ) \u2013 Maximum number of try. Qs_init ( int , default: 0.0 ) \u2013 Initial value of Q estimates. For optimistic initialisation, use positive values. record_stat ( bool , default: True ) \u2013 Records the statistics and then returns it as stat dictionary content (see Returns). Returns: Qs ( ndarray ) \u2013 A one-D array of size 'k' that contains the estimated expected action-value. stats ( dict ) \u2013 A dictionary of recorded statistics. It will be empty when 'record_stat' is False, returns empty. The followings are the keys of the recorded stats: 'rewards' : ndarray A one-D array of size 'max_try' that contains each action's reward in order. 'Ns' : ndarray A one-D array of size 'k' that contains the number of time each action were selected. 'actions' : ndarray A one-D array of size 'max_try' that contains each actions in order. greedy_upper_confidence_bound greedy_upper_confidence_bound ( k : int , bandits , c : float = 2.0 , max_try : int = 10000 , Qs_init : float = 0.0 , record_stat : bool = True , ) An upper-confidence-bound exploration/explotation algorithm. Parameters: k ( int ) \u2013 Number of arms. bandits \u2013 A k-arms bandits that returns rewards for actions in {0, 1, ..., k-1}. c ( float , default: 2.0 ) \u2013 A positive value for controling exploration degree. max_try ( int , default: 10000 ) \u2013 Maximum number of try. Qs_init ( int , default: 0.0 ) \u2013 Initial value of Q estimates. For optimistic initialisation, use positive values. record_stat ( bool , default: True ) \u2013 Records the statistics and then returns it as stat dictionary content (see Returns). Returns: Qs ( ndarray ) \u2013 A one-D array of size 'k' that contains the estimated expected action-value. stats ( dict ) \u2013 A dictionary of recorded statistics. It will be empty when 'record_stat' is False, returns empty. The followings are the keys of the recorded stats: 'rewards' : ndarray A one-D array of size 'max_try' that contains each action's reward in order. 'Ns' : ndarray A one-D array of size 'k' that contains the number of time each action were selected. 'actions' : ndarray A one-D array of size 'max_try' that contains each actions in order. k_arms_reward_creator k_arms_reward_creator ( k : int = 10 , mean : float = 0.0 , std : float = 1.0 , non_stationary : bool = False , non_stationary_std : float = 0.01 , ) Create a normal random reward model for k actions. q_i , pr K actions averages, are selected from a normal distribution N(mu, sigma^2). Each i arm will be sampled like a normal distribution N(q_i , 1) for query. Parameters: k ( int , default: 10 ) \u2013 number of arms or actions. mean ( float , default: 0.0 ) \u2013 The mean value of random q_i*. std ( float , default: 1.0 ) \u2013 The standard deviation value of random q_i*. non_stationary ( bool , default: False ) \u2013 If True, the bandit probability distribution will be non-stationary. The non-stationary distributions have a drift parameter that randomly selected from a Gaussian N(0, non_stationary_std). And on calling the bandit, it accepts a 'time' as the second parameter that shifts the mean value of the distribution based on the drift. non_stationary_std ( float , default: 0.01 ) \u2013 The standard deviation of drifts for non-stationary bandits. Returns: q_stars ( ndarray ) \u2013 One-D array of k q_i*. reward ( func ) \u2013 A function with signuture reward_i that samples from N(q_i*, 1). simulations_decorators simulations_decorators ( func ) A decorator that stacks the returns of all parallel simulations. It assumes the 'func' rturns (Qs, stats) format that bandit algorithms follow.","title":"'rl.bandits'"},{"location":"bandits_package/#rlbandits","text":"","title":"'rl.bandits'"},{"location":"bandits_package/#examples","text":"Notebook =============== K-Arms bandit algorithms.","title":"Examples:"},{"location":"bandits_package/#rl.bandits.gradient_bandit","text":"gradient_bandit ( k : int , bandits , epsilon : float = 0.1 , alpha : float = 0.1 , max_try : int = 10000 , record_stat : bool = True , ) The gradient bandit (Energy) algorithm. Parameters: k ( int ) \u2013 Number of arms. bandits \u2013 A k-arms bandits that returns rewards for actions in {0, 1, ..., k-1}. epsilon ( float , default: .1 ) \u2013 Probability of exploration. alpha ( float , default: 0.1 ) \u2013 A positive value constant step size. max_try ( int , default: 10000 ) \u2013 Maximum number of try. record_stat ( bool , default: True ) \u2013 Records the statistics and then returns it as stat dictionary content (see Returns). Returns: Hs ( ndarray ) \u2013 A one-D array of size 'k' that contains the energy of action-value. stats ( dict ) \u2013 A dictionary of recorded statistics. It will be empty when 'record_stat' is False, returns empty. The followings are the keys of the recorded stats: 'rewards' : ndarray A one-D array of size 'max_try' that contains each action's reward in order. 'Ns' : ndarray A one-D array of size 'k' that contains the number of time each action were selected. 'actions' : ndarray A one-D array of size 'max_try' that contains each actions in order.","title":"gradient_bandit"},{"location":"bandits_package/#rl.bandits.greedy_exploration","text":"greedy_exploration ( k : int , bandits , epsilon : float = 0.1 , max_try : int = 10000 , Qs_init : float = 0.0 , record_stat : bool = True , ) A greedy exploration/explotation algorithm. Parameters: k ( int ) \u2013 Number of arms. bandits \u2013 A k-arms bandits that returns rewards for actions in {0, 1, ..., k-1}. epsilon ( float , default: .1 ) \u2013 Probability of exploration. max_try ( int , default: 10000 ) \u2013 Maximum number of try. Qs_init ( int , default: 0.0 ) \u2013 Initial value of Q estimates. For optimistic initialisation, use positive values. record_stat ( bool , default: True ) \u2013 Records the statistics and then returns it as stat dictionary content (see Returns). Returns: Qs ( ndarray ) \u2013 A one-D array of size 'k' that contains the estimated expected action-value. stats ( dict ) \u2013 A dictionary of recorded statistics. It will be empty when 'record_stat' is False, returns empty. The followings are the keys of the recorded stats: 'rewards' : ndarray A one-D array of size 'max_try' that contains each action's reward in order. 'Ns' : ndarray A one-D array of size 'k' that contains the number of time each action were selected. 'actions' : ndarray A one-D array of size 'max_try' that contains each actions in order.","title":"greedy_exploration"},{"location":"bandits_package/#rl.bandits.greedy_unbiased_constant_step_size","text":"greedy_unbiased_constant_step_size ( k : int , bandits , epsilon : float = 0.1 , alpha : float = 0.1 , max_try : int = 10000 , Qs_init : float = 0.0 , record_stat : bool = True , ) An unbiased constant step exploration/explotation algorithm. Since sample averaged action-value's estimates are poorly performs on nonstationary problems, while do not produce initil bias, in this algorithm we update the step size by using a trace of rewards during the run. Parameters: k ( int ) \u2013 Number of arms. bandits \u2013 A k-arms bandits that returns rewards for actions in {0, 1, ..., k-1}. epsilon ( float , default: .1 ) \u2013 Probability of exploration. alpha ( float , default: 0.1 ) \u2013 A positive value constant step size. max_try ( int , default: 10000 ) \u2013 Maximum number of try. Qs_init ( int , default: 0.0 ) \u2013 Initial value of Q estimates. For optimistic initialisation, use positive values. record_stat ( bool , default: True ) \u2013 Records the statistics and then returns it as stat dictionary content (see Returns). Returns: Qs ( ndarray ) \u2013 A one-D array of size 'k' that contains the estimated expected action-value. stats ( dict ) \u2013 A dictionary of recorded statistics. It will be empty when 'record_stat' is False, returns empty. The followings are the keys of the recorded stats: 'rewards' : ndarray A one-D array of size 'max_try' that contains each action's reward in order. 'Ns' : ndarray A one-D array of size 'k' that contains the number of time each action were selected. 'actions' : ndarray A one-D array of size 'max_try' that contains each actions in order.","title":"greedy_unbiased_constant_step_size"},{"location":"bandits_package/#rl.bandits.greedy_upper_confidence_bound","text":"greedy_upper_confidence_bound ( k : int , bandits , c : float = 2.0 , max_try : int = 10000 , Qs_init : float = 0.0 , record_stat : bool = True , ) An upper-confidence-bound exploration/explotation algorithm. Parameters: k ( int ) \u2013 Number of arms. bandits \u2013 A k-arms bandits that returns rewards for actions in {0, 1, ..., k-1}. c ( float , default: 2.0 ) \u2013 A positive value for controling exploration degree. max_try ( int , default: 10000 ) \u2013 Maximum number of try. Qs_init ( int , default: 0.0 ) \u2013 Initial value of Q estimates. For optimistic initialisation, use positive values. record_stat ( bool , default: True ) \u2013 Records the statistics and then returns it as stat dictionary content (see Returns). Returns: Qs ( ndarray ) \u2013 A one-D array of size 'k' that contains the estimated expected action-value. stats ( dict ) \u2013 A dictionary of recorded statistics. It will be empty when 'record_stat' is False, returns empty. The followings are the keys of the recorded stats: 'rewards' : ndarray A one-D array of size 'max_try' that contains each action's reward in order. 'Ns' : ndarray A one-D array of size 'k' that contains the number of time each action were selected. 'actions' : ndarray A one-D array of size 'max_try' that contains each actions in order.","title":"greedy_upper_confidence_bound"},{"location":"bandits_package/#rl.bandits.k_arms_reward_creator","text":"k_arms_reward_creator ( k : int = 10 , mean : float = 0.0 , std : float = 1.0 , non_stationary : bool = False , non_stationary_std : float = 0.01 , ) Create a normal random reward model for k actions. q_i , pr K actions averages, are selected from a normal distribution N(mu, sigma^2). Each i arm will be sampled like a normal distribution N(q_i , 1) for query. Parameters: k ( int , default: 10 ) \u2013 number of arms or actions. mean ( float , default: 0.0 ) \u2013 The mean value of random q_i*. std ( float , default: 1.0 ) \u2013 The standard deviation value of random q_i*. non_stationary ( bool , default: False ) \u2013 If True, the bandit probability distribution will be non-stationary. The non-stationary distributions have a drift parameter that randomly selected from a Gaussian N(0, non_stationary_std). And on calling the bandit, it accepts a 'time' as the second parameter that shifts the mean value of the distribution based on the drift. non_stationary_std ( float , default: 0.01 ) \u2013 The standard deviation of drifts for non-stationary bandits. Returns: q_stars ( ndarray ) \u2013 One-D array of k q_i*. reward ( func ) \u2013 A function with signuture reward_i that samples from N(q_i*, 1).","title":"k_arms_reward_creator"},{"location":"bandits_package/#rl.bandits.simulations_decorators","text":"simulations_decorators ( func ) A decorator that stacks the returns of all parallel simulations. It assumes the 'func' rturns (Qs, stats) format that bandit algorithms follow.","title":"simulations_decorators"},{"location":"dynamic_programming_package/","text":"'rl.dynamic_programming' Examples: Notebook1:Simple square scape Notebook2: Car rental policy iteration Notebook3: Car rental value iteration =============== Dynamic programin algorithms: 1- Policy iteration. 2- Value iteration. policy_evaluation policy_evaluation ( policy : Mapping [ str , Mapping [ str , float ]], dynamics : Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ] ], states_value : Mapping [ str , float ] = None , gamma : float = 0.9 , episodic : bool = True , est_acc : float = 0.001 , max_iteration : int = 100 , ) -> Mapping [ str , float ] Iterative policy evaluation for estimating V = v_{pi}. Given a policy, this function finds the state-value function. Parameters: policy ( Mapping [ str , Mapping [ str , float ]] ) \u2013 The policy function, pi(a|s). It is a probability distribution for each action, given the state. It must be a dictianry of dictionaries {state:{action:probability}}. Deterministic polices have one and only one action for each state. dynamics ( Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ]] ) \u2013 The environment's dynamic p(s', r | s, a). It is a dictionary of dictionaries, such that its keys are tuples of (state, action) and its values are dictionaries of {(state, reward):probability}. states_value ( Mapping [ str , float ] , default: None ) \u2013 The initial values of estimating stat-value function, v(s). When it is 'None', the method initialise it. For episodic=True inputs, the initialisation create the \"TERM\" state too. It must be a dictionary of (state:value). gamma ( float , default: 0.9 ) \u2013 The discount value. It must be in (0,1]. episodic ( bool , default: True ) \u2013 Episodic tasks. If it is True, there MUST be one state that is called \"TERM\". est_acc ( float , default: 0.001 ) \u2013 The accuracy of the estimation. The iteration will stop when the difference between the current estimates and the previous one is less than est_acc. max_iteration ( int , default: 100 ) \u2013 The maximum iteration before halting the iterartive algorithm. Raise a warning in case it halts the iteration. Returns: states_value ( Mapping [ str , float ] ) \u2013 The updated stat-value function. policy_improvement policy_improvement ( policy : Mapping [ str , Mapping [ str , float ]], dynamics : Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ] ], states_value : Mapping [ str , float ], gamma : float = 0.9 , ) -> tuple [ Mapping [ str , float ], bool ] Iterative policy improvement for pi(s). Given a states-value function, this function finds the greedy improved policy. Parameters: policy ( Mapping [ str , Mapping [ str , float ]] ) \u2013 The policy function, pi(a|s). It is a probability distribution for each action, given the state. It must be a dictianry of dictionaries {state:{action:probability}}. Deterministic polices have one and only one action for each state. dynamics ( Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ]] ) \u2013 The environment's dynamic p(s', r | s, a). It is a dictionary of dictionaries, such that its keys are tuples of (state, action) and its values are dictionaries of {(state, reward):probability}. states_value ( Mapping [ str , float ] ) \u2013 The initial values of estimating stat-value function, v(s). It must be a dictionary of (state:value). gamma ( float , default: 0.9 ) \u2013 The discount value. It must be in (0,1]. Returns: states_value ( Mapping [ str , float ] ) \u2013 The updated stat-value function. is_stable ( bool ) \u2013 Is the new policy the same as the old one. policy_iteration policy_iteration ( policy : Mapping [ str , Mapping [ str , float ]], dynamics : Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ] ], states_value : Mapping [ str , float ] = None , gamma : float = 0.9 , episodic : bool = True , est_acc : float = 0.1 , l2_acc : float = 0.1 , max_evaluation_iteration : int = 100 , max_iteration : int = 100 , verbose : bool = False , ) -> Mapping [ str , float ] Iterative policy evaluation for estimating \\pi and V = v_{pi*}. Given a policy, this function finds the optimum policy and state-value functions. Parameters: policy ( Mapping [ str , Mapping [ str , float ]] ) \u2013 The policy function, pi(a|s). It is a probability distribution for each action, given the state. It must be a dictianry of dictionaries {state:{action:probability}}. Deterministic polices have one and only one action for each state. dynamics ( Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ]] ) \u2013 The environment's dynamic p(s', r | s, a). It is a dictionary of dictionaries, such that its keys are tuples of (state, action) and its values are dictionaries of {(state, reward):probability}. states_value ( Mapping [ str , float ] , default: None ) \u2013 The initial values of estimating stat-value function, v(s). When it is 'None', the method initialise it. For episodic=True inputs, the initialisation create the \"TERM\" state too. It must be a dictionary of (state:value). gamma ( float , default: 0.9 ) \u2013 The discount value. It must be in (0,1]. episodic ( bool , default: True ) \u2013 Episodic tasks. If it is True, there MUST be one state that is called \"TERM\". est_acc ( float , default: 0.1 ) \u2013 The accuracy of the estimation. The iteration will stop when the difference between the current estimates and the previous one is less than est_acc. l2_acc ( float , default: 0.1 ) \u2013 When the norm-2 difference between two consecutive state_values in a policy evaluation is smaller than l2_acc, it stops the iteration and assumes a convergence. It is usually a sign that there are two or more equally optimal policy that the iteration switch between them. max_evaluation_iteration ( int , default: 100 ) \u2013 The maximum iteration for policy evaluation. Raise a warning in case it halts the iteration. max_iteration ( int , default: 100 ) \u2013 The maximum iteration before halting the iterative algorithm of policy evaluation. Raise a warning in case it halts the iteration. Returns: policy ( Mapping [ str , Mapping [ str , float ]] ) \u2013 The converged, optimised policy. states_value ( Mapping [ str , float ] ) \u2013 The updated stat-value function. value_iteration value_iteration ( policy : Mapping [ str , Mapping [ str , float ]], dynamics : Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ] ], states_value : Mapping [ str , float ] = None , gamma : float = 0.9 , episodic : bool = True , est_acc : float = 0.001 , max_iteration : int = 1000 , verbose : bool = False , ) -> tuple [ Mapping [ str , Mapping [ str , float ]], Mapping [ str , float ] ] Iterative value iteration for estimating \\pi and V = v_{pi*}.. Given a policy, this function finds the optimum policy and state-value functions. Parameters: policy ( Mapping [ str , Mapping [ str , float ]] ) \u2013 The policy function, pi(a|s). It is a probability distribution for each action, given the state. It must be a dictianry of dictionaries {state:{action:probability}}. Deterministic polices have one and only one action for each state. dynamics ( Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ]] ) \u2013 The environment's dynamic p(s', r | s, a). It is a dictionary of dictionaries, such that its keys are tuples of (state, action) and its values are dictionaries of {(state, reward):probability}. states_value ( Mapping [ str , float ] , default: None ) \u2013 The initial values of estimating stat-value function, v(s). When it is 'None', the method initialise it. For episodic=True inputs, the initialisation create the \"TERM\" state too. It must be a dictionary of (state:value). gamma ( float , default: 0.9 ) \u2013 The discount value. It must be in (0,1]. episodic ( bool , default: True ) \u2013 Episodic tasks. If it is True, there MUST be one state that is called \"TERM\". est_acc ( float , default: 0.001 ) \u2013 The accuracy of the estimation. The iteration will stop when the difference between the current estimates and the previous one is less than est_acc. max_iteration ( int , default: 1000 ) \u2013 The maximum iteration before halting the iterartive algorithm. Raise a warning in case it halts the iteration. Returns: states_value ( Mapping [ str , float ] ) \u2013 The updated stat-value function.","title":"'rl.dynamic_programming'"},{"location":"dynamic_programming_package/#rldynamic_programming","text":"","title":"'rl.dynamic_programming'"},{"location":"dynamic_programming_package/#examples","text":"Notebook1:Simple square scape Notebook2: Car rental policy iteration Notebook3: Car rental value iteration =============== Dynamic programin algorithms: 1- Policy iteration. 2- Value iteration.","title":"Examples:"},{"location":"dynamic_programming_package/#rl.dynamic_programming.policy_evaluation","text":"policy_evaluation ( policy : Mapping [ str , Mapping [ str , float ]], dynamics : Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ] ], states_value : Mapping [ str , float ] = None , gamma : float = 0.9 , episodic : bool = True , est_acc : float = 0.001 , max_iteration : int = 100 , ) -> Mapping [ str , float ] Iterative policy evaluation for estimating V = v_{pi}. Given a policy, this function finds the state-value function. Parameters: policy ( Mapping [ str , Mapping [ str , float ]] ) \u2013 The policy function, pi(a|s). It is a probability distribution for each action, given the state. It must be a dictianry of dictionaries {state:{action:probability}}. Deterministic polices have one and only one action for each state. dynamics ( Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ]] ) \u2013 The environment's dynamic p(s', r | s, a). It is a dictionary of dictionaries, such that its keys are tuples of (state, action) and its values are dictionaries of {(state, reward):probability}. states_value ( Mapping [ str , float ] , default: None ) \u2013 The initial values of estimating stat-value function, v(s). When it is 'None', the method initialise it. For episodic=True inputs, the initialisation create the \"TERM\" state too. It must be a dictionary of (state:value). gamma ( float , default: 0.9 ) \u2013 The discount value. It must be in (0,1]. episodic ( bool , default: True ) \u2013 Episodic tasks. If it is True, there MUST be one state that is called \"TERM\". est_acc ( float , default: 0.001 ) \u2013 The accuracy of the estimation. The iteration will stop when the difference between the current estimates and the previous one is less than est_acc. max_iteration ( int , default: 100 ) \u2013 The maximum iteration before halting the iterartive algorithm. Raise a warning in case it halts the iteration. Returns: states_value ( Mapping [ str , float ] ) \u2013 The updated stat-value function.","title":"policy_evaluation"},{"location":"dynamic_programming_package/#rl.dynamic_programming.policy_improvement","text":"policy_improvement ( policy : Mapping [ str , Mapping [ str , float ]], dynamics : Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ] ], states_value : Mapping [ str , float ], gamma : float = 0.9 , ) -> tuple [ Mapping [ str , float ], bool ] Iterative policy improvement for pi(s). Given a states-value function, this function finds the greedy improved policy. Parameters: policy ( Mapping [ str , Mapping [ str , float ]] ) \u2013 The policy function, pi(a|s). It is a probability distribution for each action, given the state. It must be a dictianry of dictionaries {state:{action:probability}}. Deterministic polices have one and only one action for each state. dynamics ( Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ]] ) \u2013 The environment's dynamic p(s', r | s, a). It is a dictionary of dictionaries, such that its keys are tuples of (state, action) and its values are dictionaries of {(state, reward):probability}. states_value ( Mapping [ str , float ] ) \u2013 The initial values of estimating stat-value function, v(s). It must be a dictionary of (state:value). gamma ( float , default: 0.9 ) \u2013 The discount value. It must be in (0,1]. Returns: states_value ( Mapping [ str , float ] ) \u2013 The updated stat-value function. is_stable ( bool ) \u2013 Is the new policy the same as the old one.","title":"policy_improvement"},{"location":"dynamic_programming_package/#rl.dynamic_programming.policy_iteration","text":"policy_iteration ( policy : Mapping [ str , Mapping [ str , float ]], dynamics : Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ] ], states_value : Mapping [ str , float ] = None , gamma : float = 0.9 , episodic : bool = True , est_acc : float = 0.1 , l2_acc : float = 0.1 , max_evaluation_iteration : int = 100 , max_iteration : int = 100 , verbose : bool = False , ) -> Mapping [ str , float ] Iterative policy evaluation for estimating \\pi and V = v_{pi*}. Given a policy, this function finds the optimum policy and state-value functions. Parameters: policy ( Mapping [ str , Mapping [ str , float ]] ) \u2013 The policy function, pi(a|s). It is a probability distribution for each action, given the state. It must be a dictianry of dictionaries {state:{action:probability}}. Deterministic polices have one and only one action for each state. dynamics ( Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ]] ) \u2013 The environment's dynamic p(s', r | s, a). It is a dictionary of dictionaries, such that its keys are tuples of (state, action) and its values are dictionaries of {(state, reward):probability}. states_value ( Mapping [ str , float ] , default: None ) \u2013 The initial values of estimating stat-value function, v(s). When it is 'None', the method initialise it. For episodic=True inputs, the initialisation create the \"TERM\" state too. It must be a dictionary of (state:value). gamma ( float , default: 0.9 ) \u2013 The discount value. It must be in (0,1]. episodic ( bool , default: True ) \u2013 Episodic tasks. If it is True, there MUST be one state that is called \"TERM\". est_acc ( float , default: 0.1 ) \u2013 The accuracy of the estimation. The iteration will stop when the difference between the current estimates and the previous one is less than est_acc. l2_acc ( float , default: 0.1 ) \u2013 When the norm-2 difference between two consecutive state_values in a policy evaluation is smaller than l2_acc, it stops the iteration and assumes a convergence. It is usually a sign that there are two or more equally optimal policy that the iteration switch between them. max_evaluation_iteration ( int , default: 100 ) \u2013 The maximum iteration for policy evaluation. Raise a warning in case it halts the iteration. max_iteration ( int , default: 100 ) \u2013 The maximum iteration before halting the iterative algorithm of policy evaluation. Raise a warning in case it halts the iteration. Returns: policy ( Mapping [ str , Mapping [ str , float ]] ) \u2013 The converged, optimised policy. states_value ( Mapping [ str , float ] ) \u2013 The updated stat-value function.","title":"policy_iteration"},{"location":"dynamic_programming_package/#rl.dynamic_programming.value_iteration","text":"value_iteration ( policy : Mapping [ str , Mapping [ str , float ]], dynamics : Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ] ], states_value : Mapping [ str , float ] = None , gamma : float = 0.9 , episodic : bool = True , est_acc : float = 0.001 , max_iteration : int = 1000 , verbose : bool = False , ) -> tuple [ Mapping [ str , Mapping [ str , float ]], Mapping [ str , float ] ] Iterative value iteration for estimating \\pi and V = v_{pi*}.. Given a policy, this function finds the optimum policy and state-value functions. Parameters: policy ( Mapping [ str , Mapping [ str , float ]] ) \u2013 The policy function, pi(a|s). It is a probability distribution for each action, given the state. It must be a dictianry of dictionaries {state:{action:probability}}. Deterministic polices have one and only one action for each state. dynamics ( Mapping [ tuple [ str , str ], Mapping [ tuple [ str , float ], float ]] ) \u2013 The environment's dynamic p(s', r | s, a). It is a dictionary of dictionaries, such that its keys are tuples of (state, action) and its values are dictionaries of {(state, reward):probability}. states_value ( Mapping [ str , float ] , default: None ) \u2013 The initial values of estimating stat-value function, v(s). When it is 'None', the method initialise it. For episodic=True inputs, the initialisation create the \"TERM\" state too. It must be a dictionary of (state:value). gamma ( float , default: 0.9 ) \u2013 The discount value. It must be in (0,1]. episodic ( bool , default: True ) \u2013 Episodic tasks. If it is True, there MUST be one state that is called \"TERM\". est_acc ( float , default: 0.001 ) \u2013 The accuracy of the estimation. The iteration will stop when the difference between the current estimates and the previous one is less than est_acc. max_iteration ( int , default: 1000 ) \u2013 The maximum iteration before halting the iterartive algorithm. Raise a warning in case it halts the iteration. Returns: states_value ( Mapping [ str , float ] ) \u2013 The updated stat-value function.","title":"value_iteration"},{"location":"simulations_package/","text":"'rl.simulations' Simulation methods for examining Reinforcment Learning algorithms. parallel parallel ( func , simulations : int = 32 , concurrent : int = 32 , func_uniqe_index : bool = True , func_args : tuple = None , ) Execute the func simulteniously on parrallel processes. Parameters: func ( function ) \u2013 A function that will be called simulteniously. simulations ( int , default: 32 ) \u2013 The number of total function calls. concurrent ( int , default: 32 ) \u2013 The number of concurrency of parallel processes. func_uniqe_index ( bool , default: True ) \u2013 If True, the iteration number will be send as the first argument to the function. func_args ( tuple ( obj ) , default: None ) \u2013 The argument(s) on calling func. Returns: list \u2013 The list of returns of all func calls.","title":"'rl.simulations'"},{"location":"simulations_package/#rlsimulations","text":"Simulation methods for examining Reinforcment Learning algorithms.","title":"'rl.simulations'"},{"location":"simulations_package/#rl.simulations.parallel","text":"parallel ( func , simulations : int = 32 , concurrent : int = 32 , func_uniqe_index : bool = True , func_args : tuple = None , ) Execute the func simulteniously on parrallel processes. Parameters: func ( function ) \u2013 A function that will be called simulteniously. simulations ( int , default: 32 ) \u2013 The number of total function calls. concurrent ( int , default: 32 ) \u2013 The number of concurrency of parallel processes. func_uniqe_index ( bool , default: True ) \u2013 If True, the iteration number will be send as the first argument to the function. func_args ( tuple ( obj ) , default: None ) \u2013 The argument(s) on calling func. Returns: list \u2013 The list of returns of all func calls.","title":"parallel"}]}