{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Reinforcement Learning documentation! Description Examples that describe and display different concept in reinforcement learning","title":"Reinforcement Learning documentation!"},{"location":"#reinforcement-learning-documentation","text":"","title":"Reinforcement Learning documentation!"},{"location":"#description","text":"Examples that describe and display different concept in reinforcement learning","title":"Description"},{"location":"bandits_package/","text":"'rl.bandits' K-Arms bandit algorithms. greedy_ee greedy_ee ( k , bandits , epsilon : float = 0.1 , max_try : int = 10000 , Qs_init : float = 0.0 , record_stat : bool = True , ) A greedy exploration/explotation algorithm. Parameters: bandits \u2013 A k-arms bandits that returns rewards for actions in {0, 1, ..., k-1}. epsilon ( float , default: .1 ) \u2013 Probability of exploration. max_try ( int , default: 10000 ) \u2013 Maximum number of try. Qs_init ( int , default: 0.0 ) \u2013 Initial value of Q estimates. For optimistic initialisation, use positive values. record_stat ( bool , default: True ) \u2013 Records the statistics and then returns it as stat dictionary content (see Returns). Returns: Qs ( ndarray ) \u2013 A one-D array of size 'k' that contains the estimated expected action-value. stats ( dict ) \u2013 A dictionary of recorded statistics. It will be empty when 'record_stat' is False, returns empty. The followings are the keys of the recorded stats: 'rewards' : ndarray A one-D array of size 'max_try' that contains each action's reward in order. 'Ns' : ndarray A one-D array of size 'k' that contains the number of time each action were selected. 'actions' : ndarray A one-D array of size 'max_try' that contains each actions in order. greedy_ucb greedy_ucb ( k , bandits , c : float = 2.0 , max_try : int = 10000 , Qs_init : float = 0.0 , record_stat : bool = True , ) An upper-confidence-bound exploration/explotation algorithm. Parameters: bandits \u2013 A k-arms bandits that returns rewards for actions in {0, 1, ..., k-1}. c ( float , default: 2.0 ) \u2013 A positive value for controling exploration degree. max_try ( int , default: 10000 ) \u2013 Maximum number of try. Qs_init ( int , default: 0.0 ) \u2013 Initial value of Q estimates. For optimistic initialisation, use positive values. record_stat ( bool , default: True ) \u2013 Records the statistics and then returns it as stat dictionary content (see Returns). Returns: Qs ( ndarray ) \u2013 A one-D array of size 'k' that contains the estimated expected action-value. stats ( dict ) \u2013 A dictionary of recorded statistics. It will be empty when 'record_stat' is False, returns empty. The followings are the keys of the recorded stats: 'rewards' : ndarray A one-D array of size 'max_try' that contains each action's reward in order. 'Ns' : ndarray A one-D array of size 'k' that contains the number of time each action were selected. 'actions' : ndarray A one-D array of size 'max_try' that contains each actions in order. k_arms_reward_creator k_arms_reward_creator ( k : int = 10 , mean : float = 0.0 , std : float = 1.0 ) Create a normal random reward model for k actions. q_i , pr K actions averages, are selected from a normal distribution N(mu, sigma^2). Each i arm will be sampled like a normal distribution N(q_i , 1) for query. Parameters: k ( int , default: 10 ) \u2013 number of arms or actions. mean ( float , default: 0.0 ) \u2013 The mean value of random q_i*. std ( float , default: 1.0 ) \u2013 The standard deviation value of random q_i*. Returns: q_stars ( ndarray ) \u2013 One-D array of k q_i*. reward ( func ) \u2013 A function with signuture reward_i that samples from N(q_i*, 1). simulations_decorators simulations_decorators ( func ) A decorator that stacks the returns of all parallel simulations. It assumes the 'func' rturns (Qs, stats) format that bandit algorithms follow.","title":"'rl.bandits'"},{"location":"bandits_package/#rlbandits","text":"K-Arms bandit algorithms.","title":"'rl.bandits'"},{"location":"bandits_package/#rl.bandits.greedy_ee","text":"greedy_ee ( k , bandits , epsilon : float = 0.1 , max_try : int = 10000 , Qs_init : float = 0.0 , record_stat : bool = True , ) A greedy exploration/explotation algorithm. Parameters: bandits \u2013 A k-arms bandits that returns rewards for actions in {0, 1, ..., k-1}. epsilon ( float , default: .1 ) \u2013 Probability of exploration. max_try ( int , default: 10000 ) \u2013 Maximum number of try. Qs_init ( int , default: 0.0 ) \u2013 Initial value of Q estimates. For optimistic initialisation, use positive values. record_stat ( bool , default: True ) \u2013 Records the statistics and then returns it as stat dictionary content (see Returns). Returns: Qs ( ndarray ) \u2013 A one-D array of size 'k' that contains the estimated expected action-value. stats ( dict ) \u2013 A dictionary of recorded statistics. It will be empty when 'record_stat' is False, returns empty. The followings are the keys of the recorded stats: 'rewards' : ndarray A one-D array of size 'max_try' that contains each action's reward in order. 'Ns' : ndarray A one-D array of size 'k' that contains the number of time each action were selected. 'actions' : ndarray A one-D array of size 'max_try' that contains each actions in order.","title":"greedy_ee"},{"location":"bandits_package/#rl.bandits.greedy_ucb","text":"greedy_ucb ( k , bandits , c : float = 2.0 , max_try : int = 10000 , Qs_init : float = 0.0 , record_stat : bool = True , ) An upper-confidence-bound exploration/explotation algorithm. Parameters: bandits \u2013 A k-arms bandits that returns rewards for actions in {0, 1, ..., k-1}. c ( float , default: 2.0 ) \u2013 A positive value for controling exploration degree. max_try ( int , default: 10000 ) \u2013 Maximum number of try. Qs_init ( int , default: 0.0 ) \u2013 Initial value of Q estimates. For optimistic initialisation, use positive values. record_stat ( bool , default: True ) \u2013 Records the statistics and then returns it as stat dictionary content (see Returns). Returns: Qs ( ndarray ) \u2013 A one-D array of size 'k' that contains the estimated expected action-value. stats ( dict ) \u2013 A dictionary of recorded statistics. It will be empty when 'record_stat' is False, returns empty. The followings are the keys of the recorded stats: 'rewards' : ndarray A one-D array of size 'max_try' that contains each action's reward in order. 'Ns' : ndarray A one-D array of size 'k' that contains the number of time each action were selected. 'actions' : ndarray A one-D array of size 'max_try' that contains each actions in order.","title":"greedy_ucb"},{"location":"bandits_package/#rl.bandits.k_arms_reward_creator","text":"k_arms_reward_creator ( k : int = 10 , mean : float = 0.0 , std : float = 1.0 ) Create a normal random reward model for k actions. q_i , pr K actions averages, are selected from a normal distribution N(mu, sigma^2). Each i arm will be sampled like a normal distribution N(q_i , 1) for query. Parameters: k ( int , default: 10 ) \u2013 number of arms or actions. mean ( float , default: 0.0 ) \u2013 The mean value of random q_i*. std ( float , default: 1.0 ) \u2013 The standard deviation value of random q_i*. Returns: q_stars ( ndarray ) \u2013 One-D array of k q_i*. reward ( func ) \u2013 A function with signuture reward_i that samples from N(q_i*, 1).","title":"k_arms_reward_creator"},{"location":"bandits_package/#rl.bandits.simulations_decorators","text":"simulations_decorators ( func ) A decorator that stacks the returns of all parallel simulations. It assumes the 'func' rturns (Qs, stats) format that bandit algorithms follow.","title":"simulations_decorators"},{"location":"simulations_package/","text":"'rl.simulations' Simulation methods for examining Reinforcment Learning algorithms. parallel parallel ( func , simulations : int = 32 , concurrent : int = 32 , func_uniqe_index : bool = True , func_args : tuple = None , ) Execute the func simulteniously on parrallel processes. Parameters: func ( function ) \u2013 A function that will be called simulteniously. simulations ( int , default: 32 ) \u2013 The number of total function calls. concurrent ( int , default: 32 ) \u2013 The number of concurrency of parallel processes. func_uniqe_index ( bool , default: True ) \u2013 If True, the iteration number will be send as the first argument to the function. func_args ( tuple ( obj ) , default: None ) \u2013 The argument(s) on calling func. Returns: list \u2013 The list of returns of all func calls.","title":"'rl.simulations'"},{"location":"simulations_package/#rlsimulations","text":"Simulation methods for examining Reinforcment Learning algorithms.","title":"'rl.simulations'"},{"location":"simulations_package/#rl.simulations.parallel","text":"parallel ( func , simulations : int = 32 , concurrent : int = 32 , func_uniqe_index : bool = True , func_args : tuple = None , ) Execute the func simulteniously on parrallel processes. Parameters: func ( function ) \u2013 A function that will be called simulteniously. simulations ( int , default: 32 ) \u2013 The number of total function calls. concurrent ( int , default: 32 ) \u2013 The number of concurrency of parallel processes. func_uniqe_index ( bool , default: True ) \u2013 If True, the iteration number will be send as the first argument to the function. func_args ( tuple ( obj ) , default: None ) \u2013 The argument(s) on calling func. Returns: list \u2013 The list of returns of all func calls.","title":"parallel"}]}